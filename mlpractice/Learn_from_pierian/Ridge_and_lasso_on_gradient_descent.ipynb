{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33ad3d2b-8a2b-42a6-8664-ca317b87885b",
   "metadata": {},
   "source": [
    "# How Ridge Shrinks Coefficient in Gradient Descent\n",
    "> X and $\\beta$ are the vector representaions of features and coefficient\n",
    "\n",
    "- $X^TX$ is a $p\\times p$ matrix. It is invertible if the features are not perfectly colinear.\n",
    "- $(X^TX)^{-1}$ helps to undo the effect of colinearity between the features. Normalizes the contribution of each features to the final solution.\n",
    "- $X^Ty$ represents the correlation between the features and target variable $y$\n",
    "- putting them together:\n",
    "  $$ \\beta = (X^TX)^{-1}(X^Ty) $$\n",
    "- gives the coefficients of linear regression\n",
    "\n",
    "\n",
    "# Ridge Regression:\n",
    "$$ Error =  ||y-X\\beta||^2 + \\lambda ||\\beta||^2$$\n",
    "$$ SSR = ||y-X\\beta||^2$$\n",
    "$$ Penalty = \\lambda ||\\beta||^2 $$\n",
    "- gradient of the SSR:\n",
    "$$ J(\\beta) = -2X^T(y-X\\beta)$$\n",
    "- gradient of Penalty\n",
    "$$ J(\\beta) = 2\\lambda \\beta $$\n",
    "\n",
    "- Combining them we get:\n",
    "$$ J(\\beta) = -2X^T(y-X\\beta) + 2\\lambda \\beta $$\n",
    "The gradient descent aims to minimmize this cost function. This happens as\n",
    "$$\\nabla J(\\beta) = -2X^T(y-X\\beta) + 2\\lambda \\beta $$\n",
    "\n",
    "The value of beta is updated until convergance as:\n",
    "$$ \\beta \\leftarrow \\beta - \\alpha \\nabla J(\\beta) $$\n",
    "$$ \\beta \\leftarrow \\beta - \\alpha [-2X^T(y-X\\beta) + 2\\lambda \\beta] $$\n",
    "$$\\beta \\leftarrow \\beta + 2\\alpha X^T(y - X\\beta) - 2\\alpha\\lambda\\beta$$\n",
    "$$\\beta \\leftarrow \\beta (1 - 2\\alpha\\lambda) + 2\\alpha X^T(y - X\\beta)$$\n",
    "\n",
    "Here, the term $2\\alpha X^T(y - X\\beta)$ reduces the sum of squared residuals by least square method and the term $\\beta(1-2\\alpha\\lambda)$ reduces \\beta by some extra penalty that we tune with Ridge regression\n",
    "\n",
    "- Here, we can see that for each update, $\\beta$ is shrinked by $2\\alpha\\lambda$\n",
    "\n",
    "# Lasso Regression\n",
    "- Unlike ridge regression which uses quadratic penalty, the L1 (lasso regression) adds linear penalty term.\n",
    "- The linear penalty has more aggressive shrinkage effect on small coefficient.\n",
    "- #### Sparsity:\n",
    "    - Lasoo regression can drive some coefficient to exactly zero.\n",
    "    - For small coefficients, the penalty is strong enough to make them zero\n",
    "    - For larger coefficient, the penalty shrinks them but does not necessarily make them zero\n",
    "\n",
    "- The cost function for lasso regression:\n",
    "$$ J(\\beta) = ||y-X\\beta||^2 + \\lambda||\\beta||$$\n",
    "    - where, $||\\beta||$ is the sum of absolute value of coefficients.\n",
    "\n",
    "- For gradient calculation:\n",
    "$$ \\nabla J(\\beta) = −2X^T(y−X\\beta)+\\lambda(\\beta)$$\n",
    "- Update rule, update until convergence as:\n",
    "$$ \\beta \\leftarrow \\beta - \\alpha \\nabla J(\\beta) $$\n",
    "$$ \\beta \\leftarrow \\beta - \\alpha[−2X^T(y−X\\beta)+\\lambda(\\beta)]$$\n",
    "$$ \\beta \\leftarrow \\beta + 2\\alpha X^T(y−X\\beta)- \\alpha\\lambda(\\beta)$$\n",
    "    - here, the term $2\\alpha X^T(y−X\\beta)$ aims to optimize the residuals\n",
    "    - the term $\\alpha\\lambda(\\beta)$ aims to shrink the coefficients based on their absolute value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0fa46a-deeb-4f90-89b9-dac293f49388",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
