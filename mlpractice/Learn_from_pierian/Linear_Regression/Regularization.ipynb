{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21432f1d-3770-46aa-bbc1-af178056ab32",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Regularizaion](#Regularization)\n",
    "    - [Types of Regularization](#Types-of-Regularization)\n",
    "      - [1. L1 Regularization](#1.-L1-Regularization-(Lasso-Regression))\n",
    "      - [2. L2 Regularization](#2.-L2-Regularization-(Ridge-Regression))\n",
    "    - [Regularization Conclusion](#Conclusion)\n",
    "- [Revisit on Feature Scaling](#Revisit-on-Feature-Scaling)\n",
    "# Regularization\n",
    "Regularization is a way to reduce model overfitting. It requires some additional bias and search for optimal penalty hyperparameter.\n",
    "#### How it is done:\n",
    "- Minimizing model complexity\n",
    "- penalizing loss function \n",
    "- Reducing model overfitting (by adding some bias to reduce variance)\n",
    "\n",
    "#### How overfitting happens:\n",
    "- It happens when the model is too flexible, and the training process adapts too much to the training data, thereby losing predictive accuracy on new test data. The causing factor is high variance low bias\n",
    "- <b><u>Intuition</u></b>:the curve is too smooth passing through every data points in training set\n",
    "\n",
    "# Types of Regularization\n",
    "1. L1 Regularization (or Lasso Regression)\n",
    "2. L2 Regularization (or Ridge Regression)\n",
    "3. Elastic Net Regularization (combination of L1 and L2)\n",
    "\n",
    "# 1. L1 Regularization (Lasso Regression)\n",
    "- Adds a penalty equal to the absolute value of the magnitude of the coefficients to the loss function (aka cost function)\n",
    "- It limits the size of coefficients in the regression equation\n",
    "- It can yield sparse models where some coefficients can be zero (In polynomial regression as we saw some coefficients were very small that they're almost zero, it'll treat them as zero eliminating the coefficient)\n",
    "$$ L1 = \\sum_{i=0}^{m-1}(y_i - \\hat y_i)^2 + \\lambda \\sum_{j=0}^{n-1} |\\beta_j| $$\n",
    "$$ which\\ is: $$\n",
    "$$ L1 = SSR + \\lambda \\sum_{j=0}^{n-1} |\\beta_j| $$\n",
    "    - here, SSR = Sum of Squared Residuals\n",
    "    - $\\lambda$ is a hyperparameter\n",
    "# 2. L2 Regularization (Ridge Regression)\n",
    "- Adds a penalty equal to the squared of the magnitude of coefficients\n",
    "- All the coefficients are shrunk by same factor but doesnot necessarily eliminate them.\n",
    "\n",
    "$$ L2 = \\sum_{i=0}^{m-1}(y_i - \\hat y_i)^2 + \\lambda \\sum_{j=0}^{n-1} (\\beta_j)^2$$\n",
    "$$ which\\ is: $$\n",
    "$$ L2 = SSR + \\lambda \\sum_{j=0}^{n-1} (\\beta_j)^2 $$\n",
    "\n",
    "# Conclusion\n",
    "- In regularization we are just adding some penalty term to the the error that we're trying to minimize\n",
    "- $\\lambda$ is the hyperparameter. $\\lambda = 0$ is same as not performing any regularization and becomes just a Sum of Squared Residuals (SSR) \n",
    "\n",
    "# Revisit on Feature Scaling\n",
    "- Algorithim like gradient descent and KNN (which relys on distance metric) requires feature scaling to perform well\n",
    "- In gradient descent, the features with large scale will have their coefficient updated faster than the coefficient of small scaled features. Scaled features will allow gradient descent to converge efficiently.\n",
    "- There are some algorithms in ML where feature scaling will have no effect. (Regression trees, decision trees, random forest etc.)\n",
    "- Generally, decision tree based algorithms will have no effect with feature scaling\n",
    "> If we scale the training features, we'll have to scale the unseen data too before feeding it to the model\n",
    "\n",
    "- Improves coefficient interpreatability meaning we can relate and compare between coefficients and their impact.\n",
    "- It causes great increase in model performance\n",
    "\n",
    "> If we're not sure if we need to scale or not, we can scale it anyway. It has no drawback and doesnot affect the data set in any conditions. Only thing to remember is to scale the unseen data (with the same scaling factors as in trained model) before feeding it to the model\n",
    "\n",
    "#### Ways to scale features\n",
    "1. <b>Standardization</b>: Rescale data to have mean $(\\mu) = 0$ and standard deviation $(\\sigma) = 1$. It is also known as Z-score normalization\n",
    "      $$x_{1,scaled} = \\frac{x_1 - \\mu_1}{\\sigma_1}$$\n",
    "2. <b>Normalization</b>: Rescale all the data values to be between $0\\ and\\ 1$\n",
    "      $$x_{1,scaled} = \\frac{x_1 - x_{min}}{x_{max} - x_{min}}$$\n",
    "> While performing featuring scaling mean, standard deviation, min and max should be calculated for train data. Using all the data set(both train and test) will cause information leakage to the training data. We can use .fit() and .transform() method in scikit learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1f43f3-4bd4-4a73-9f23-7d4732a3e748",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "772501a7-ba00-4890-b51f-e3eddcbb991a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c644f9f-ffca-466d-974a-598fe367c812",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
