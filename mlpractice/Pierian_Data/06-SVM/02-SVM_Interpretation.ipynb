{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46b0cc6c-c22e-42d3-8011-5cc960b2dc2f",
   "metadata": {},
   "source": [
    "# Maximal Margin Classifier\n",
    "- The maximal margin classifier is suitable for the condition where the classes are well separated from each other\n",
    "- In such case the position for the hyperplane is determined on the basis of maximum margin between the support vectors\n",
    "$$ \\max\\limits_{\\beta_0, \\beta_1, \\dots, \\beta_p, M} M$$\n",
    "$$ Subject\\ to\\ \\sum_{j=1}^{p} \\beta_j^2 = 1 $$\n",
    "$$y_i (\\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\dots + \\beta_px_{ip} )\\geq M \\qquad \\forall \\quad i = 1, \\dots, n $$\n",
    "\n",
    "- Here\n",
    "- $M$ is the width of the margin\n",
    "- $y_i$ is the class label with values $+1\\ and\\ -1$\n",
    "- $(\\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\dots + \\beta_px_{ip} ) = 0$ defines the hyperplane\n",
    "- $(\\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\dots + \\beta_px_{ip} ) > 0$ defines positive class\n",
    "- $(\\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\dots + \\beta_px_{ip} ) < 0$ defiens negative class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ab476c-6cf3-4b12-84fe-a8c2006bb8ea",
   "metadata": {},
   "source": [
    "# Support Vector Classifier\n",
    "- Support Vector Classifier is suitable when the classes are not separable by a single hyperplane\n",
    "- SVC allows soft margins meaning some data points are allowed for missclassification introducing error terms $\\epsilon$ and tuining parameter $C$\n",
    "\n",
    "$$ \\max\\limits_{\\beta_0, \\beta_1, \\dots, \\beta_p, \\epsilon_1, \\dots \\epsilon_n, M} M$$\n",
    "$$ Subject\\ to\\ \\sum_{j=1}^{p} \\beta_j^2 = 1 $$\n",
    "$$y_i (\\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\dots + \\beta_px_{ip} )\\geq M (1-\\epsilon_i),\\qquad \\epsilon \\geq 0,\\qquad \\sum_{i=1}^n \\epsilon_i \\leq C $$\n",
    "\n",
    "- Here\n",
    "- $\\epsilon$ is the errors occured by missclassification\n",
    "- $C$ is non-negative tuining parameter, it sets the limit on how much error we are willing to accept\n",
    "- Sum of errors is less or equal to $C$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f27552-5b98-4125-bd4f-7d162fa0b524",
   "metadata": {},
   "source": [
    "# SVM (Support Vector Machine)\n",
    "- Support Vector machine is suitable for the case when both MMC and SVC are not able to set the single hyperplane to separate calsses.\n",
    "- SVM allows to the projection of linear dataset into higher dimension dataset using kernel\n",
    "- Then an appropriate hyperplane can be generated at that dimension with SVC\n",
    "\n",
    "### Non-Linear Decision Boundary classification\n",
    "- For the sake of simplicity, lets do it with 2nd degree polynomial with no interaction terms\n",
    "$$ \\max\\limits_{\\beta_0, \\beta_{11}, \\beta_{12}, \\dots, \\beta_{p1}, \\beta_{p2}, \\epsilon_1, \\dots \\epsilon_n, M} M$$\n",
    "$$Subject\\ to \\qquad \\sum_{j=1}^p \\sum_{k=1}^2 \\beta_{jk}^2 = 1$$\n",
    "$$\\qquad y_i \\left(\\beta_0 + \\sum_{j=1}^p \\beta_{j1}x_{ij} + \\sum_{j=1}^p \\beta_{j2}x_{ij}^2 \\right) \\geq M (1-\\epsilon_i), \\qquad  \\epsilon \\geq 0,\\qquad \\sum_{i=1}^n \\epsilon_i \\leq C$$\n",
    "\n",
    "- Here,\n",
    "- $\\beta_{j1}$ is the coefficient for the original linear term\n",
    "- $\\beta_{j2}$ is the coefficient for the transformed polynomial term\n",
    "- $x_{ij}$ is the original linear term\n",
    "- $x_{ij}^2$ is the transformed polynomial term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cbe375-3fad-4ad5-b470-3a1b822edbad",
   "metadata": {},
   "source": [
    "# Kernels\n",
    "- In case of linarly unseparable classes, kernels are used to transform the linear dataset into higher dimensional data set\n",
    "- Most common kernels are **Linear Kernel**, **Polynomial Kernels**, **Radial Basis Function (RBF) or Gaussian Kernel**, **Sigmoid Kernel**\n",
    "- Mapping the data set directly with these kernel function without explicitly computing the data set in that dimension is known as **_Kernel Trick_**\n",
    "\n",
    "## Use of kernel in training set\n",
    "- Before applying SVM model to the training set, if the dataset is not linearly separable, a RBF or polynomial kernel can be used to directly transform linear data to the higher dimension\n",
    "\n",
    "## Use of Kernel in prediction\n",
    "- Since, the SVM uses only support vectors for the prediction the kernel function are applied to all the support vectors.\n",
    "- Based on the result follwing things are considered for the prediction,\n",
    "    - if the dot product is 0, the new point is on the decision boundary itself (in such case other things are considered such as always assign to a particualr class or apply some particular bias to decide class)\n",
    "    - if the dot product is positive, the new point is on the class with which support vector it results in positive dot product (the intensity is also considered, the greater the value of dot product the similar the points are)\n",
    "    - if the dot product results in negative, the data point lies in the opposite class.\n",
    "\n",
    "### <u>Decision Function in SVM (i.e for prediction)</u>\n",
    "$$f(x) =\\beta_0 + \\sum_{i=1}^n \\alpha_i \\langle x, x_i \\rangle$$\n",
    "\n",
    "- Here, $\\alpha_i$ is the Lagrange multiplier\n",
    "- $\\alpha_i \\neq 0$ only for the support vectors\n",
    "- $x$ is a test point and $x_i$ are the training points\n",
    "\n",
    "Since, $\\alpha_i$ is zero for non support vector data points, $x_i$ can be considered as support vectors and the formula can be rewritten as:\n",
    "$$ f(x) = \\beta_0 + \\sum_{i \\in S} \\alpha_i \\langle x, x_i \\rangle$$\n",
    "\n",
    "- Any kernel function applied to the above function can replace the term $\\langle x, x_i \\rangle$ with the kernel term\n",
    "\n",
    "## Linear Kernel\n",
    "- Linear kernel is suitable for only the prediction. Because the data set is already a linear set, no transformation is required\n",
    "$$K(x_1, x_2) = x_1 \\cdot x_2 $$\n",
    "\n",
    "- Here, $x_1$ is the test point and $x_2$ is some support vector\n",
    "\n",
    "## Polynomial Kernel\n",
    "$$k(x_i,x'_i) = ( x_i \\cdot x'_i + c)^d$$\n",
    "- Sometimes the dot product may be represented as inner product $\\langle x_i, x'_i \\rangle$\n",
    "\n",
    "## Radial Basis Function\n",
    "$$k(x_i, x'_i) = exp \\left( -\\frac{||x_i - x'_i||^2}{2 \\sigma^2} \\right) $$\n",
    "$$ Or,$$\n",
    "$$k(x_i, x'_i) = exp(-\\gamma ||x_i - x'_i||^2)$$\n",
    "\n",
    "- Here,\n",
    "- $\\gamma = \\frac{1}{2\\sigma^2}$\n",
    "- The $\\gamma$ parameter defines the width of bell-shaped curve, here, larger the value of gamma the narrower the bell\n",
    "- $\\sigma$ is the scale parameter (standard deviation) of the gaussian function, $\\sigma^2$ is variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31bcfab-e163-40fe-adce-5a23b6d31a14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
